\documentclass[class=article, crop=false]{standalone}
\usepackage[subpreambles=true]{standalone}
\usepackage{import}
\usepackage{ebproof}
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{verbatim}
\usepackage{a4wide}
\usepackage[super]{nth}

\usepackage{subcaption}

\ifstandalone
\input{../src/efflang.tex}
\else
\input{./src/efflang.tex}
\fi

\ifstandalone
\input{../src/scheme.tex}
\else
\input{./src/scheme.tex}
\fi

\renewcommand{\leadsto}{\rightsquigarrow}
\newcommand{\dmid}{\ \parallel \ }

\newcommand{\effFalse}{\mathbf{false}}
\newcommand{\effTrue}{\mathbf{true}}
\newcommand{\effLeft}{\mathbf{Left}\ }
\newcommand{\effRight}{\mathbf{Right\ }}
\newcommand{\effFun}{\mathbf{fun}\ }
\newcommand{\effHandler}{\mathbf{handler}\ }
\newcommand{\effVal}{\mathbf{val}\ }
\newcommand{\effWith}{\mathbf{with}\ }
\newcommand{\effHandle}{\ \mathbf{handle}\ }
\newcommand{\effIf}{\mathbf{if}\ }
\newcommand{\effThen}{\ \mathbf{then}\ }
\newcommand{\effElse}{\ \mathbf{else}\ }
\newcommand{\effAbsurd}{\mathbf{absurd}\ }
\newcommand{\effMatch}{\mathbf{match}\ }
\newcommand{\effLet}{\mathbf{let}\ }
\newcommand{\effIn}{\ \mathbf{in}\ }
\newcommand{\effRec}{\mathbf{rec}\ }
\newcommand{\effEffect}{\mathbf{effect}\ }
\newcommand{\effFinally}{\mathbf{finally}\ }
\newcommand{\effOp}{\mathtt{op}}
\newcommand{\effPerform}{\mathbf{perform}\ }
\newcommand{\tto}{\twoheadrightarrow}

\newcommand{\handlerType}{\Rightarrow}
\newcommand{\boolType}{\mathtt{bool}}
\newcommand{\unitType}{\mathtt{unit}}
\newcommand{\emptyType}{\mathtt{empty}}

\newcommand{\defEq}{\stackrel{\text{def}}{=}}

\newcommand{\cek}[1]{\langle #1 \rangle}
\newcommand{\secd}[1]{\langle #1 \rangle}

\begin{document}

This chapter of my dissertation contains a tutorial introduction to the programming language
Eff. It also discusses continuations and their relationships with two abstract machines, and
talks about some of the software engineering aspects of the project.

\section{A quick Eff tutorial}

Eff is a programming language based on algebraic effect handlers.
Eff resembles OCaml in that if we removed OCaml's \cite{ocaml-website} side-effecting operations and
re-created them by \emph{simulating} them with a more general notion of exception
(specifically, resumable exceptions)
then we would get Eff as the result. In Eff, \emph{effect} is used to mean
resumable exception. This tutorial will show how printing and non-determinism
can be realised as effects in this context.

To implement effects and effect handlers Eff uses the following keywords which are
not part of OCaml at the time of writing\footnote{Although a branch of OCaml, namely Multicore OCaml
is just getting merged in the main OCaml branch so we might see some of these keywords in OCaml in the future.}: 
\verb|effect|, \verb|perform|, \verb|handler|, \verb|continue|, \verb|with-handle| and \verb|finally|.
What follows is a brief introduction of these features by using the
unmissable Hello World example.

\subsection{Effects}

Effects lie in the heart of Eff (hence the name). A programmer can define
effects using the \verb|effect| keyword, by specifying the effect's name
(which must be capitalized just like OCaml variant tags) and the type of the effect.
An effect definition can be seen on line 1 of \autoref{first-example}. Effects always have a type
of $A \tto B$ for some types $A$ and $B$.

In the introduction of this tutorial I said that effects are resumable exceptions. The careful
reader might notice that the type $A \tto B$ is similar to the function type $A \to B$ but
it is quite different from usual types of exceptions. For instance, in SML \cite{milner1997definition}
one would expect to see declarations representing errors like \verb|exception Error of string| (an error which
can carry an error message with itself could be declared like this) but would not expect to talk about the \emph{return type} of
an exception, as these can never return in SML. However, in Eff, due to the resumable nature of effects
this makes sense.

\subsection{Performing effects}

Effects behave a bit like functions as we could see from their types above.
Once an effect of type $A \tto B$ is declared we can invoke it
using the \verb|perform| keyword and by providing an argument of type $A$.
The type of the resulting expression is $B$.

\begin{figure}[hbt!]
  \lstinputlisting[caption={Printing is an effect taking a string as its argument and returning unit as a result.},
  label=first-example, language=efflang]{../code_examples/first_program.eff}
\end{figure}

The \verb|perform| keyword is similar to \verb|raise| in OCaml or to \verb|throw|
in Java in that when we perform an effect the control will be given to an effect
handler (this would be an exception handler in OCaml or Java) or the program
will terminate with an exception if no handler can handle the effect.

Don't be confused by how an effect is constructed. Effects are not values in
the Eff language. That is, \verb|Print "Hello World"| would be meaningless in
itself, just like the exception \verb|Failure "hd"| would be meaningless in OCaml
if we did not raise it. The situation is similar here but effects are \emph{performed} and not raised.

In Java, if we throw an exception without introducing a \verb|try-catch| block handling
it, the exception will rise to the toplevel and will cause our program to crash.
This is exactly the result \autoref{first-example} would produce
as we didn't surround line 3 with the \verb|try-catch| block equivalent
of Eff, which is a \verb|with-handle| block. However, to do this, we need to be
able to declare \emph{effect handlers} first.

\subsection{Handlers}

Handlers give meaning to effects: this is what programmers can use to specify
what they mean by an effect, such as \verb|Print| in \autoref{first-example}.
Using handlers we can make the code evaluate properly. Right now it always
crashes on line 3 and does not continue executing the rest of the program.
This is hardly what we desire from \verb|Print|.

Before we do this, we must think about how we would \emph{simulate printing} in a pure functional
language. One way of doing it would be to implement all functions of type $A \to B$ to
return something of type $B \times \mathtt{string}$ instead, i.e., the result the function would return in
its original form \emph{together} with a message it prints. Such a transformation would be
a tedious manual task to do for \emph{all} our functions, not to mention how we would need to
unbox the actual result of a function from a tuple every time we wished to access it. Fortunately,
handlers happen to alleviate this problem.

Handlers are first-class citizens (i.e., values) in Eff and they show some similarities
with SML's \verb|case| statement. A handler can be declared with the \verb|handler| keyword
and by specifying so-called \emph{cases}:

\lstinputlisting[caption={A handler definition consisting of 2 cases: a value case and an effect case.}, label=handler-keyword, language=efflang]{../code_examples/print_handler.eff}
To complete our Hello World example we must be able to \emph{apply} this handler. This is done
using a \emph{with-handle block} as illustrated on \autoref{hello-world}.

\lstinputlisting[caption={The Hello World example in Eff. The code above evaluates to the pair \texttt{((), "Hello World!")}.}, label=hello-world, language=efflang]{../code_examples/hello_world.eff}

The handler on \autoref{handler-keyword} implements printing. It uses two types of cases as described below.

A \emph{value case} has the form $\mathtt{val}\ x \to c$. This is a rule that describes how to handle the
\emph{return value} of a computation.
It takes the return value of a computation enclosed by a with-handle block, binds it to the
identifier $x$ and performs the computation $c$.
In \autoref{handler-keyword} we are simply saying that if the computation handled by this
handler has finished computing without invoking any effects, then we are returning a pair containing
the result of the computation and the empty string (this reflects that nothing was printed).

An \emph{effect case} is a generalisation of SML's \verb|handle| or Java's \verb|catch|. 
It has the form $\mathtt{effect}\ \mathtt{op}\ e\ k \to c$, where the \verb|effect| keyword
plays the r√¥le of Java's catch, \verb|op| is an effect name, $e$ is an expression, $k$ is
an identifier to be used as the name for the continuation and $c$ is a computation.
The continuation $k$ is then possible to \emph{resume} with the \verb|continue| keyword
and an argument to the continuation.
Note that when an effect of type $A \tto B$ is handled,
we get \emph{read access} to its argument $e$ of type $A$ and we can resume its continuation $k$ if we provide
$k$ a value of type $B$ (this is why we resume the continuation by giving it a
\verb|()| value in \autoref{handler-keyword}---\verb|Print| is of type $\mathtt{string} \tto \mathtt{unit}$).

% TODO:  and a finally rule. The effect rule first resumes the rest of the program to obtain its result and its output. ``Printing'' out \texttt{message} is simulated by string concatenation; we prepend \texttt{message} to the output of the rest of the program.
As handlers are values, they have their own type of the form $A \Rightarrow B$.
Handler types are similar to the $\to$ function types but they work on computations
rather than on values. We can see why this is so if we think about what \verb|prepending_print_handler|
is doing in our example. Used with a \verb|with-handle| block it transforms any computation
of type $A$ to one of the type $A \times \mathtt{string}$. One might think of handlers
as transformations on computations. The handler on \autoref{hello-world} is of
type $\mathtt{unit} \Rightarrow \mathtt{unit} \times \mathtt{string}$.

\subsubsection{The finally case}

There is an extra case we did not mention, that of the \emph{finally case}. A finally case is
invoked after the computation in a with-handle block finished evaluating and after all other
handler cases finished evaluating.

Finally cases are just syntactic sugar for let wrappers around with-handle blocks.
That is, $\textbf{finally } r \to f$ does the same as $\textbf{let } r \leftarrow \textbf{with } v \textbf{ handle } c \textbf{ in } f$.
Finally rules were introduced to avoid having to use such inconvenient let wrappers \cite{bauer2015programming}.
To illustrate a use-case of this feature consider the following code snippet where the result of a physics simulation is returned in
Kelvin but we need this in Fahrenheit to display it on a user interface:
\lstinputlisting[
  caption={},
  label=handler-keyword, 
  language=efflang]
  {../code_examples/celsius_fahrenheit.eff}
Instead of using a let-wrapper every time to convert to Fahrenheit we could use the same handler $h$ but extend it with a finally rule
that does this conversion and hence avoids code duplication:
\lstinputlisting[
  caption={},
  label=handler-keyword, 
  language=efflang]
  {../code_examples/celsius_fahrenheit2.eff}

A handler can have any number of effect cases (even zero) but at most one finally case and at most one value case.
Value cases and finally cases are optional. When they are avoided they are assumed to be identities (i.e.,
\verb|val x -> x| or \verb|finally x -> x| respectively).

\subsection{The tutorial illustrated}

Imagining how the execution happens in the Hello World example is not trivial. The aim of the following illustration
is to help with this.
\begin{figure}[ht]
\begin{subfigure}{.25\textwidth}
  \centering
  \resizebox{1\textwidth}{!}{
    \subimport{../figures/}{exception_control_flow.tex}
  }
  \caption{Without a handler an effect behaves like an exception. Here it is raised to the toplevel and causes the program to crash.}
  \label{fig:exception}
\end{subfigure}
\begin{subfigure}{.72\textwidth}
  \centering
  \resizebox{1\textwidth}{!}{
    \subimport{../figures/}{handler_hello_world.tex}
  }
  \caption{Effect handlers generalise exceptions. In contrast to (a) we see that here we resume the computation twice and enter the same handler twice.}
  \label{fig:exception}
\end{subfigure}
\end{figure}

\section{Continuations and control operators}
\label{sec:continuations}

Although continuations came up briefly in the previous section I did not explain what they are in detail.
This section is devoted to this because continuations are playing a crucial r√¥le in Eff and in what follows
in this dissertation.

% Continuations are also sometimes thought about as expressions with holes in them.

% Continuations were first discovered in 1964 by van Wijngaarden \cite{reynolds1993discoveries}.

\subsection{CPS in Scheme}

Continuations can represent an arbitrary program point in the execution of a program and thus one can say
that continuations are always there, whenever a program executes. But where? A programming style called
\emph{continuation passing style} is a style of programming where every function of $n$ arguments is
rewritten to an extended version taking $n+1$ arguments, the last of which is commonly named $k$ and
represents a continuation.

Consider the Scheme code snippet below which uses this style to redefine the built-in binary addition and
multiplication functions of Scheme as ternary functions \verb|+k| and \verb|*k| with an extra argument \verb|k|.
Also note how the expression \verb|(1 + 2) * (3 + 4)| is written in this style.
\begin{lstlisting}[language=scheme]
(define return (lambda x x))

(define (+k a b k) (k (+ a b))) 
(define (*k a b k) (k (* a b)))

#| Compute (1 + 2) * (3 + 4) |#
(+k 1 2 (lambda (sum1)
  (+k 3 4 (lambda (sum2)
    (*k sum1 sum2 return)))))
\end{lstlisting}
We see that continuation passing style makes the order of the operations explicit as well as it makes it
obvious \emph{what} the continuation is. However, writing code in this style is a bit troublesome and hence
it is mostly used as an intermediate representation of programs in compilers -- not something humans interact with.

\subsection{Scheme's call/cc}

Scheme is interesting from the point of view of continuations because it has the
\verb|call-with-current-continuation| built-in operation which is conventionally referred to as \verb|call/cc|
\footnote{A similarly obscure control operator is Peter Landin's J operator which predates \texttt{call/cc} by almost a decade.
It was discovered shortly after Peter Landin described SECD machines for the first time.}.\emph{The call/cc operator takes a single argument. This argument is a function which itself takes a continuation as an argument.}

If one wished to use continuations as first-class values in a Scheme program he wouldn't have to write everything in
continuation passing style like above. A similar piece of code can be written with call/cc:
\begin{lstlisting}[language=scheme]
(define call/cc call-with-current-continuation)

#| Compute (1 + 2) * (3 + 4) |#
(* (+ 3 4) (call/cc (lambda (k) (k (+ 1 2)))))

#| The following line evaluates to 12 too |#
(* (+ 3 4) (call/cc (lambda (k) (+ 100 (k (+ 1 2))))))
\end{lstlisting}
But looking at line 7 we see that the continuation
here returns to the toplevel. Call/cc captures the continuation
of the \emph{whole} program and \emph{does not} return to the program
point the continuation was called from. This is different from
how this is done in Eff.

\subsection{Delimited continuations}

Eff uses delimited continuations. These continuations do not represent the rest of
the \emph{whole} program. They represent the continuation of the computation enclosed by a
with-handle block.
\begin{lstlisting}[language=efflang]
let x =
	with prepending_handler handle
		perform (Print "Hello");
		perform (Print "World");
		()
in 42;;
\end{lstlisting}
In the code above \verb|x| takes the value \verb|((), "Hello World")| rather than
\verb|42| which one would expect with Scheme's call/cc. The fact that continuations
are delimited also makes it possible to return to the program point a continuation was called
from. Hence we can think about delimited continuations simply as functions.

% check this out
% https://en.wikipedia.org/wiki/Delimited_continuation

\section{Eff formally}

Eff is described in \cite{bauer2015programming}.

\subsection{Syntax}

% syntactic difference between pure expressions and possibly effectful computations
% iota?
% implementations can avoid the syntactic difference
% talk about monads here

In the abstract syntax of Eff there is a distinction between pure expressions and computations which are possibly effectful.
\footnote{The reader familiar with monads can might discover some similarities between \emph{val} and \emph{return} as well
as similarities between monadic type constructors and computations.}

\begin{figure}
Effect declarations
$$ \effEffect E : A \tto B $$
Expressions
$$ e ::= x \mid
  \effTrue \mid
  \effFalse \mid
  () \mid
  (e_1, e_2) \mid
  \effLeft e \mid
  \effRight e \mid
  \effFun x \mapsto c \mid
  h $$
Handlers
$$ h ::= \effHandler \effVal x \mapsto c_v\ \overline{\|\ \effEffect E_i\ x\ k \to c_{\effOp_i} } \dmid \effFinally x \mapsto c_f $$
Computations
\begin{align*}
  c ::= \ &\effVal e \mid
    \effLet x = c_1 \effIn c_2 \mid
    \effLet \effRec f\ x = c_1 \effIn c_2 \mid e_1\ e_2 \mid \\
    & \effIf e \effThen c_1 \effElse c_2 \mid
    \effMatch e\ \effWith \effLeft x \mapsto c_l \dmid \effRight y \mapsto c_r \mid\\
    & \effMatch e\ \effWith (f, s) \mapsto c \mid \effPerform (E\ e) \mid
    \effWith e \effHandle c
\end{align*}
\caption{Abstract syntax of Eff}
\label{fig:eff-syntax}
\end{figure}

Most terms should be familiar to the reader by now. However, there are some differences between the concrete and
the abstract syntax of Eff. Notably, the use of the \emph{val} keyword is omitted many times from the concrete
syntax because for a programmers it might be tiresome if they have to keep track whether they are writing an
expression or a computation at the moment. The \emph{continue} keyword is missing from the abstract syntax too.
However, our ability to resume computations did not vanish.
As we saw in the previous section, delimited continuations are just simply functions reifying a continuation.
Hence we can handle resumptions as function application. Anyhow, to ease the understanding of the semantic rules
I use the notation $y.c$ for a delimited continuation which resumes the computation $c$ after receiving $y$.

\subsection{Semantics}

This section presents the small step operational semantics of Eff (see \autoref{fig:small-step-semantics}). The spotlight is on
possibly effectful computations and on their reduction rules given by the relation $\leadsto$. I focus on the Eff specific
transition steps here. The other transition rules are standard and can be found in Appendix X.\footnote{Comment to Alan: I was always wondering
how $y.e$, i.e., a delimited continuation here is very similar to blocking communication + guarded commands.}

\begin{figure}
$$
\begin{prooftree}
\hypo{}
\infer1[(\textsc{If-True})]{\effIf \effTrue \effThen c_1 \effElse c_2 \leadsto c_1}
\end{prooftree}
\quad
\begin{prooftree}
  \hypo{}
  \infer1[(\textsc{If-False})]{\effIf \effFalse \effThen c_1 \effElse c_2 \leadsto c_2}
\end{prooftree}
$$

$$
\begin{prooftree}
  \hypo{}
  \infer1[(\textsc{Fun-App})]{(\effFun x \mapsto c)\ e \leadsto c[e/x]}
\end{prooftree}
\quad
\begin{prooftree}
  \hypo{}
  \infer1[(\textsc{Match-Left})]{\effMatch (\effLeft e)\ \effWith \effLeft x \mapsto c_1 \dmid \effRight x \mapsto c_2  \leadsto c_1[e/x]}
\end{prooftree}
$$

$$
\begin{prooftree}
  \hypo{}
  \infer1[(\textsc{Match-Right})]{\effMatch (\effRight e)\ \effWith \effLeft x \mapsto c_1 \dmid \effRight x \mapsto c_2  \leadsto c_2[e/x]}
\end{prooftree}
$$

$$
\begin{prooftree}
  \hypo{}
  \infer1[(\textsc{Match-Prod})]{\effMatch (e_1, e_2)\ \effWith (f,s) \mapsto c \leadsto c[e_1/f, e_2/s]}
\end{prooftree}
$$

$$
\begin{prooftree}
  \hypo{c_1 \leadsto c_1'}
  \infer1[(\textsc{Let-Step})]{\effLet x = c_1 \effIn c_2 \leadsto \effLet x = c_1' \effIn c_2}
\end{prooftree}
\quad
\begin{prooftree}
  \hypo{}
  \infer1[(\textsc{Let-Val})]{\effLet x = (\effVal e) \effIn c \leadsto c[e/x]}
\end{prooftree}
$$

$$
\begin{prooftree}
  \hypo{}
  \infer1[(\textsc{Let-Effect})]{\effLet x = E(e, y.c_1) \effIn c_2 \leadsto E(e, y.\effLet x = c_1 \effIn c_2)}
\end{prooftree}
$$

$$
\begin{prooftree}
  \hypo{}
  \infer1[(\textsc{Let-Rec})]{\effLet \effRec f\ x = c_1 \effIn c_2 \leadsto c_2[(\effFun x \mapsto \effLet \effRec f\ x = c_1 \effIn c_1)/f]}
\end{prooftree}
$$

$$
\begin{prooftree}
  \hypo{\kappa \text{ is current delimited continuation}}
  \infer1[(\textsc{Perform})]{\effPerform (E\ e) \leadsto E(e, \kappa)}
\end{prooftree}
$$

$$
\begin{prooftree}
  \hypo{c \leadsto c'}
  \infer1[(\textsc{Handle-Step})]{\effWith e \effHandle c \leadsto \effWith e \effHandle c'}
\end{prooftree}
$$

$$
\begin{prooftree}
  \hypo{h \stackrel{\text{def}}{=} (\effHandler \effVal x \mapsto c_v \dmid \dots)}
  \infer1[(\textsc{Handle-Val})]{\effWith h \effHandle (\effVal e) \leadsto c_v[e/x]}
\end{prooftree}
$$

$$
\begin{prooftree}
  \hypo{h \stackrel{\text{def}}{=} (\effHandler \dots \overline{\dmid \effEffect E_i\ x\ k \mapsto c_i} \dmid \dots)}
  \hypo{\exists i. E_i = E}
  \infer2[(\textsc{Handle-Eff-Match})]{\effWith h \effHandle E(e, y.c) \leadsto c_i[e/x, (y.c)/k]}
\end{prooftree}
$$

$$
\begin{prooftree}
  \hypo{h \stackrel{\text{def}}{=} (\effHandler \dots \overline{\dmid \effEffect E_i\ x\ k \mapsto c_i} \dmid \dots)}
  \hypo{\forall i. E_i \neq E}
  \infer2[(\textsc{Handle-Eff-Rise})]{\effWith h \effHandle E(e, y.c) \leadsto E( e, y. \effWith h \effHandle c)}
\end{prooftree}
$$

\caption{Small step operational semantics of core Eff}
\label{fig:small-step-semantics}
\end{figure}

The rules for if statements, function application, match statements and the rules \textsc{Handle-Step}, \textsc{Let-Step} and \textsc{Let-Rec}
are standard. The other \textsc{Let-*} and \textsc{Handle-*} rules need more explanation:
\begin{table}
  \setlength{\tabcolsep}{0.5em}
  \renewcommand{\arraystretch}{1.4}
  \begin{tabular}{l|p{10cm}}
  Rule & Comment \\
  \hline \hline
  \textsc{Let-Val} & If a computation \emph{returns} an expression $e$, it is bound to the identifier $x$ in the computation $c$. \\ \hline
  \textsc{Let-Effect} & A computation $c$ can also reduce to an effect $\iota\#op\ e\ (y.c_1)$, where $e$ is the argument of the effect
    constructor and $(y.\ c_1)$ is a delimited continuation representing the program point $c$'s execution was stopped at. This delimited continuation can be
    used later to resume $c$. \\ \hline
  \textsc{Handle-Val} & If a computation in a with-handle block with handler $h$ reduces to $\effVal e$, then the
    \emph{value case} of $h$ (i.e., $\effVal x \mapsto c_v$) is invoked by substituting $e$ for $x$ in $c_v$. \\ \hline
  \textsc{Handle-Effect} & This rule tells us how to look up the \emph{closest matching handler} if a computation reduces to an effect.
    If no such handler exists then the program terminates with an exception. The recursive definition of $ocs_{\iota\#op}$ describes how
    to look up a \emph{matching case} in a handler. Note how a \emph{new} continuation $(y.\kappa\ y)$ is created from $\kappa$ when we reach $nil$, i.e.,
    when no case matches the effect in question.
  \end{tabular}
  \caption{Explanation of the Eff specific rules of the small step semantics}
  \label{tab:rule-explanation}
\end{table}

\subsection{Types}

The types of Eff are as follows:
\begin{figure}
  $$A ::= 
    b \mid
    \mathtt{bool} \mid
    \mathtt{unit} \mid
    \mathtt{empty} \mid
    A * B \mid
    A + B \mid
    A \to B \mid
    A \tto B \mid
    A \Rightarrow B$$
  \caption{The types of Eff.}
  \label{fig:eff-types}
\end{figure}
where $b$ stands for other built-in types (e.g., my implementation includes built-in types such as ints, reals and strings) which are not
particularly interesting here.

\subsubsection{Type checking}

Again, I omit standard typing rules and focus on the Eff specific ones. For the sake of completeness Appendix Y contains the other rules.
There are two separate typing relations denoted $\vdash_e$ and $\vdash_c$ for expressions and computations respectively.
Type environments are denoted $\Gamma$ and are maps from variable names to types. We assume the existence of a set $\Sigma_E$ containing
all effect declarations of the form $E_i : A_i \tto B_i$.

\begin{figure}
  $$
  \begin{prooftree}
    \hypo{x : A \in \Gamma}
    \infer1[(\textsc{T-Var})]{\Gamma \vdash_e x : A}
  \end{prooftree}
  $$
  $$
  \begin{prooftree}
    \hypo{}
    \infer1[(\textsc{T-True})]{\Gamma \vdash_e \effTrue : \boolType}
  \end{prooftree}
  $$
  $$
  \begin{prooftree}
    \hypo{}
    \infer1[(\textsc{T-False})]{\Gamma \vdash_e \effFalse : \boolType}
  \end{prooftree}
  $$
  $$
  \begin{prooftree}
    \hypo{}
    \infer1[(\textsc{T-Unit})]{\Gamma \vdash_e () : \unitType}
  \end{prooftree}
  $$
  $$
  \begin{prooftree}
    \hypo{\Gamma \vdash_e e_1 : A}
    \hypo{\Gamma \vdash_e e_2 : B}
    \infer2[(\textsc{T-Pair})]{\Gamma \vdash_e (e_1, e_2) : A * B}
  \end{prooftree}
  $$
  $$
  \begin{prooftree}
    \hypo{\Gamma \vdash_e e : A}
    \infer1[(\textsc{T-SumLeft})]{\Gamma \vdash_e \effLeft e : A + B}
  \end{prooftree}
  $$
  $$
  \begin{prooftree}
    \hypo{\Gamma \vdash_e e : B}
    \infer1[(\textsc{T-SumRight})]{\Gamma \vdash_e \effRight e : A + B}
  \end{prooftree}
  $$
  $$
  \begin{prooftree}
    \hypo{x : A, \Gamma \vdash_c c : C}
    \infer1[(\textsc{T-Fun})]{\Gamma \vdash_e (\effFun x \mapsto c) : A \to C}
  \end{prooftree}
  $$

  \begin{align*}
    {\begin{prooftree}
      \hypo{x : A, \Gamma \vdash_c c_v : C}
      \hypo{\forall i.\ e_i : A_i, k_i : B_i, \Gamma \vdash_c c_i : C}
      \hypo{x : C, \Gamma \vdash_c c_f : D}
      \infer3[(\textsc{T-Handler})]{\Gamma \vdash_e h : A \handlerType C}
    \end{prooftree}}\\
    \text{ where }
    h \defEq (
      \effHandler
        \effVal x \mapsto c_v
        \overline{\dmid \effEffect E_i\ e_i\ k_i \mapsto c_i} \dmid
        \effFinally x \mapsto c_f)
    \text{ and } \forall i.\ E_i : A_i \tto B_i \in \Sigma_E
  \end{align*}
  
  \caption{Type checking rules for Eff expressions}
  \label{fig:type-checking-expressions}
\end{figure}

\begin{figure}
  $$
  \begin{prooftree}
    \hypo{\Gamma \vdash_e e : A}
    \infer1[(\textsc{T-Val})]{\Gamma \vdash_c \effVal e : A}
  \end{prooftree}
  $$
  $$
  \begin{prooftree}
    \hypo{\Gamma \vdash_c c_1 : A}
    \hypo{x : A, \Gamma \vdash_c c_2 : B}
    \infer2[(\textsc{T-Let})]{\Gamma \vdash_c (\effLet x = c_1 \effIn c_2) : B}
  \end{prooftree}
  $$
  $$
  \begin{prooftree}
    \hypo{x : A, f : A \to B \Gamma \vdash_c c_1 : B}
    \hypo{f : A \to B, \Gamma \vdash_c c_2 : C}
    \infer2[(\textsc{T-LetRec})]{\Gamma \vdash_c (\effLet \effRec f\ x = c_1 \effIn c_2) : C}
  \end{prooftree}
  $$
  $$
  \begin{prooftree}
    \hypo{\Gamma \vdash_e e_1 : A \to B}
    \hypo{\Gamma \vdash_e e_2 : A}
    \infer2[(\textsc{T-FunApp})]{\Gamma \vdash_c e_1\ e_2 : B}
  \end{prooftree}
  $$
  $$
  \begin{prooftree}
    \hypo{\Gamma \vdash_e e : \boolType}
    \hypo{\Gamma \vdash_c c_1 : A}
    \hypo{\Gamma \vdash_c c_2 : A}
    \infer3[(\textsc{T-If})]{\Gamma \vdash_c \effIf e \effThen c_1 \effElse c_2 : A}
  \end{prooftree}
  $$
  $$
  \begin{prooftree}
    \hypo{\Gamma \vdash_e e : A + B}
    \hypo{x : A, \Gamma \vdash_c c_l : C}
    \hypo{x : B, \Gamma \vdash_c c_r : C}
    \infer3[(\textsc{T-MatchSum})]{\effMatch e\ \effWith \effLeft x \mapsto c_l \dmid \effRight x \mapsto c_r : C}
  \end{prooftree}
  $$
  $$
  \begin{prooftree}
    \hypo{\Gamma \vdash_e e : A * B}
    \hypo{f : A, s: B, \Gamma \vdash_c c_l : C}
    \infer2[(\textsc{T-MatchProd})]{\effMatch e\ \effWith (f,s) \mapsto c : C}
  \end{prooftree}
  $$
  $$
  \begin{prooftree}
    \hypo{E : A \tto B \in \Sigma_E}
    \hypo{\Gamma \vdash_e e : A}
    \infer2[(\textsc{T-Perform})]{\effPerform (E\ e) : B}
  \end{prooftree}
  $$
  $$
  \begin{prooftree}
    \hypo{\Gamma \vdash_e e : A \handlerType B}
    \hypo{\Gamma \vdash_c c : A}
    \infer2[(\textsc{T-WithHandle})]{\effWith e \effHandle c : B}
  \end{prooftree}
  $$

  \caption{Type checking rules for Eff computations}
  \label{fig:type-checking-computations}
\end{figure}


The distinction between pure expressions and effectful computations allows for the use of an \emph{effect system} with
these rules. For instance, if one were to extend these rules with \emph{row-typing} one could
keep track of what kind of side effecting computations occur in different parts of the program. This is can aid
reasoning about programs, help us to discover more optimisations by using type and effect information and to 
ensure that programs have certain safety properties.
These are nice to have features and are outside of the scope of this dissertation. My implementation uses these simplified
set of rules.

\section{Abstract machines by example}

A purist would look at rule \textsc{(Perform)} in \autoref{fig:small-step-semantics} and would think that the phrasing of the
hypothesis is troublesome. I actually think that this ambiguity reflects well the challenge in this project. Consider for instance
implementing an interpreter on the source terms of a language X. One's first instinct is that this is a trivial task: all we need
to do is to write a recursive evaluator function, something of this sort:
\begin{lstlisting}[language=caml]
let rec eval = function
| Plus(e1, e2) -> eval(e1) + eval(e2)
| ...
\end{lstlisting}
Indeed, this works for most programming languages (ones that do not have control operators like continuations) and this was my first
attempt when writing a \nth{0} prototype in the very early stages of the project. I started to struggle when I reached the point I had to
deal with continuations, as I needed the ability to access $\kappa$ any time in the evaluation of a program. But evaluators of this kind
do not lend themselves easily to such maneuvers.

Hence this section is concerned with two types of abstract machines which are capable of doing what is described above. I will illustrate
how they do this on a toy language and by the end of this section we will be able discover a correspondence between the \emph{runtime representation}
of continuations in these machines and the \emph{continuation passing style} Scheme example from \autoref{sec:continuations}.

These two machines are CEK and SECD machines.
They both interpret source terms directly and this makes it easy to implement them (a CEK implementation of Eff is discussed in the
Implementation chaper).

Consider the following Toy Language (TL), which is the lambda calculus with natural numbers as values and with built-in operations
like addition and multiplication:
$$ t ::= x \mid n \mid \lambda x. t \mid t_0\ t_1 \mid t_0 + t_1 \mid t_0 * t_1 $$
together with our dummy example from \autoref{sec:continuations} in TL:
$$ \mathtt{plus} = \lambda x. \lambda y.\ x + y $$
$$ \mathtt{times} = \lambda x. \lambda y.\ x * y $$
$$ \mathtt{times}\ (\mathtt{plus}\ 1\ 2)\ (\mathtt{plus}\ 3\ 4).$$
What follows is a brief description of CEK and SECD machines for TL illustrated on this example.

\subsection{CEK machines}

As the name suggests, the state of CEK machines consists of three components: \emph{control} (C), \emph{environment} (E) and \emph{kontinuation} (K).
Control is simply the program to be evaluated in its abstract syntax tree form. Environment is a mapping from variable names to values (in our case, the
natural numbers) and kontinuation is a \emph{stack of closures} representing the rest of the program to be interpreted at any given time.
Hence a configuration of the machine is characterised by a three-tuple $\cek{C, E, K}$.

\begin{figure}
  Initialisation:
  $$ t \to \cek{t, \{\}, nil} $$
  Transition rules:
  $$ \cek{x, E, (y.t, E') :: K} \to \cek{E(x), E, K} $$
  $$ \cek{v, E, (y.t, E') :: K} \to \cek{t, E'[y \mapsto v], K} $$
  $$ \cek{(\lambda y. t_1)\ t_2, E, K} \to \cek{t_2, E, (y.t_1, E) :: K} $$
  $$ \cek{t_1 + t_2, E, K} \to \cek{\mathtt{add}(t_1, t_2), E, K} $$
  $$ \cek{t_1 * t_2, E, K} \to \cek{\mathtt{mult}(t_1, t_2), E, K} $$
  $$ \cek{\mathtt{op}(t_1, t_2), E, K} \to \cek{t_1, E, (y.\mathtt{CONT1}(y, \mathtt{op}, t_2), E) :: K} \text{ for a \emph{fresh} $y$} $$
  $$ \cek{\mathtt{CONT1}(x, \mathtt{op}, t_2), E, K} \to \cek{t_2, E, (y.\mathtt{CONT2}(x, \mathtt{op}, y), E) :: K} \text{ for a \emph{fresh} $y$} $$
  $$ \cek{\mathtt{CONT2}(x, \mathtt{op}, y), E, K} \to \cek{v, E, K} \text{ where } v = \mathtt{op}(x, y) $$
  Termination:
  $$ \cek{x, E, nil} \to E(x) $$
  $$ \cek{v, E, nil} \to v $$

  \caption{CEK machine transition rules for TL.}
  \label{fig:cek-tl}
\end{figure}

We see that whenever we reduce a term to a value $v$, a continuation $(y.t)$ is popped off the K stack, $v$ is bound to the
variable $y$ in $E'$ (this is the environment we need to restore to be able to interpret $t$ in the same context we stopped
evaluating its parent expression) and then $t$ is resumed. We need to use two types of continuations for TL as it has
binary operators $+$ and $*$. The name \texttt{CONT1} denotes that one of the operands is reduced to a value and one
remains to be reduced before the actual operation can be performed. \texttt{CONT2} is used to represent the state when
both of the operand are values and the operation \texttt{op} can be performed (be that the built-in \texttt{add} or \texttt{mult}).

We see that a $\effPerform$ operation would be easier to implement in this representation than in the na√Øve way. Furthermore,
we should notice that a CPS transformation is done on the terms ``on the fly'' during the evaluation and we benefit from this
because this avoids us (programmers) having to make the continuations explicit in our programs by writing them in continuation passing
style.

\subsection{The SECD machine}

SECD machines originate from Landin's famous \emph{The mechanical evaluation of expressions} \cite{landin-secd}.
Their state is characterised by four components, namely: a \emph{stack} (S), an \emph{environment} (E), a \emph{control} (C) and a \emph{dump} (D).
The first three components are standard. Here the r√¥le of dump is similar to the r√¥le of K in the CEK machine. It is a list of
three tuples, each three tuple consisting of the other three components, i.e it has the form $(S, E, C)$. The dump
embodies a list of saved execution contexts we can return to if we wish.

\begin{figure}
  Initialisation:
  $$ t \to \secd{nil, \{\}, [t], nil} $$
  Transition rules:
  $$ \secd{S, E, x :: C, D} \to \secd{E(x) :: S, E, C, D} $$
  $$ \secd{v :: S, E, nil, (S', E', C') :: D} \to \secd{v :: S', E', C', D} $$
  $$ \secd{S, E, (t_1\ \mathtt{op}\ t_2 :: C, D} \to \secd{S, E, t_1 :: t_2 :: \mathtt{op} :: C, D} $$
  $$ \secd{S, E, (t_1\ t_2) :: C, D} \to \secd{S, E, t_2 :: t_1 :: \mathtt{ap} :: C, D} $$
  $$ \secd{(y, E', t) :: v :: S, E, \mathtt{ap} :: C, D} \to \secd{nil, E'[y \mapsto v], [t], (S, E, C) :: D} $$
  $$ \secd{v_2 :: v_1 :: S, E, \mathtt{op} :: C, D} \to \secd{v :: S, E, C, D} \text{ for } v = \mathtt{op}(v_1, v_2) $$
  Termination:
  $$ \secd{v :: S, E, nil, nil} \to v $$
\end{figure}

We see that although this machine works on abstract syntax trees, it is still a bit lower level than the CEK machine in that
it makes a stack explicit and the way it processes (disassembles) the tree is very similar to compiling. In control, we end
up with a list of smaller trees and SECD instructions. With just another step we can turn this into a stack machine and this
is what I will do in the Implementation chapter.

\section{Illustration/The Big Picture?}

\begin{figure}
  \begin{subfigure}{\textwidth}
    \centering
    \resizebox{1\textwidth}{!}{
      \subimport{../figures/}{naive.tex}
    }
    \caption{Imagine that a $\effPerform$ operation is issued at position 2 in the tree. As the na√Øve approach only keeps track of
    the continuations implicitly we cannot represent the red arrow.}
    \label{fig:naive}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
    \centering
    \resizebox{1\textwidth}{!}{
      \subimport{../figures/}{cek.tex}
    }
    \caption{The content of K in the CEK machine. The CEK machine represents the continuation the same way our continuations were nested in each other
    in the Scheme example.}
    \label{fig:naive}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
    \centering
    \resizebox{1\textwidth}{!}{
      \subimport{../figures/}{secd.tex}
    }
    \caption{The SECD machine does not need to pass values around between closures. The input values for all the closures will already reside on
    the stack when the execution gets to the point where it needs to invoke the continuation. This is why the representation of a continuation here
    looks much more like an instruction stream.}
    \label{fig:naive}
  \end{subfigure}

  \caption{Illustration of the different approaches on the same example.}
  \label{fig:naive-cek-secd}
\end{figure}

\section{Software engineering}

\subsection{Licensing}

It is important to mention that the project fulfills licensing criteria. The dependencies of this project are
listed below with their licenses:
\begin{itemize}
\item The OCaml toplevel, version 4.06.0, GNU Lesser General Public License (LGPL) version 2.1
\item \emph{menhir} parser generator, version 20190924, GNU Library General Public License version 2
\item \emph{Alcotest}, version 0.8.5, ISC license
\item OPAM (OCaml Package Manager), version 2.0.4, GNU Lesser General Public License (LGPL) version 2.1
\item \emph{Core} standard library by Jane Street, v0.11.3, MIT license
\end{itemize}
To the best of my knowledge these licenses allow the way I am using the software mentioned above.

\subsection{Starting point}

The starting point of my project didn't change since the writing of the project proposal [TODO: ref. appendix here]:
\begin{itemize}
  % knowledge backround
  \item I read briefly about Eff and algebraic effect handlers before starting my project.
  \item I used SML and OCaml before to implement simple interpreters for toy programming languages
    and to experiment with functional programming. This is my first big project where I use functional programming.
  \item I also had a look at the OCaml ecosystem to check if it has a stable build system, if testing frameworks are available
    and whether lexer and parser generators exist.
  \item I had basic familiarity with utility tools such as version control, various IDEs and Unix-like command line interfaces.
\end{itemize}

% a lot of knowledge needed to be acquired
% CEK interpreter has 2 goals: one to test my understanding and the other to give a baseline

\subsection{Development methodology}

I chose \emph{test-driven development} (TDD) \cite{beck2003test} as my project's software development methodology.
The essence of TDD is that programmers work in quick iterative cycles whereby they write failing tests first and then
implement the next feature that makes the failing test cases pass.
Always having an \emph{automated} test suite at hand avoids trial and error testing which would be both error prone and
time consuming.
The reasons for choosing this methodology was three-fold.

Firstly, the nature of my project is such that the
correctness of any code written is very well-defined as I can take the semantics of Eff
as a baseline and given a piece of code it is always possible to tell whether it is
interpreted correctly or not. This makes it possible to write tests upfront.

Second, the structure of my project imposed an order on the parts to be delivered. This is
because every module of my project (apart from the front end) depends on a previous one (see
\autoref{fig:project-structure}) and hence on its
correctness. As TDD is also known as 
``a way of managing fear during programming'' it came handy that I could write code with confidence knowing
that the code I have written before is tested. Other advantages were the ability to notice
quickly if a code change broke some already implemented feature and the ability to refactor
code with confidence (which I had to do frequently---from summer internships I knew that my coding style
is such that this will be unavoidable if I want to work with a maintainable codebase).

Third, my success criterion was a correct working implementation of a compiler so tests were
crucial to demonstrate that success criteria were fulfilled.

\begin{figure}[htb]
  \centering
  \resizebox{0.85\textwidth}{!}{
  \begin{tikzpicture}
    \node[draw,inner sep=10pt] (LEX) at (0,6) {Lexer};
    \node[draw,inner sep=10pt] (PAR) at (0,4) {Parser};
    \node[draw,inner sep=10pt] (TYPE) at (0,2) {Type and effect checking};
    \node[draw,inner sep=10pt] (MID) at (5, 2) {Middle-end};
    \node[draw,inner sep=10pt] (COMP) at (9,2) {Compiler};
    \node[draw,inner sep=10pt] (CEK) at (0,0) {CEK interpreter};
    \node[draw,inner sep=10pt] (YOYO) at (9,0) {YoYo machine};
    
    \draw[->] (LEX) -- (PAR);
    \draw[->] (PAR) -- (TYPE) node[midway, right] {Parse trees};
    \draw[->] (TYPE) -- (CEK) node[midway, right] {ASTs};
    \draw[->] (TYPE) -- (MID) node[midway, above] {ASTs};
    \draw[->] (MID) -- (COMP) node[midway, above] {IR};
    \draw[->] (COMP) -- (YOYO) node[midway, right] {byte code};
  \end{tikzpicture}}
  \caption{Structure of the project}
  \label{fig:project-structure}
\end{figure}

\section{Summary}
This section summed up the theory background which was needed to complete this dissertation.
It also shows the preparation I undertook in the software engineering aspects of the project.

\begin{itemize}
  \item I briefly introduced the programming language Eff and talked about its characteristic features. I showed
    code examples demonstrating how handlers can be used.
  \item I explained the idea of a continuation, how it represents an arbitrary program point in the
    execution of a program and how it allows advanced control flow to be implemented.
  \item I gave an overview of control operators (including Landin's J operator), why they were studied in the 
    early days of programming languages and their r√¥le in the development of different virtual machines.
  \item I talked about the small-step operational semantics of core Eff as well as the typing of Eff terms.
  \item I mentioned two abstract machines which can implement the structured \emph{non-local
    flow of control} continuations make possible. The key challenge these machines solve is that
    they can \emph{save and restore} execution contexts in an appropriate way whenever non-local control flow occurs.

    A CEK machine does this by keeping a stack of continuations (K), while the SECD machine does this by using
    a so-called dump (D). The importance of these machines will become clearer in the Implementation section where I describe the
    Yoyo virtual machine for Eff which has similarities with both of these abstract machines.
  \item I stated my starting point, the timeline of my project and discussed the development methodology used
    to write the code and the testing strategy to assess its correctness.
\end{itemize}


\begin{comment}
\subsection{Eff}

This subsection gives a short introduction to Eff with some code examples.
The precise syntax and semantics of the language will be defined later,
we aim for an intuitive introduction here.

\subsubsection{Effects}

There is an important distinction between syntactic and semantic elements in the language. Effect declarations belong to syntax.

An effect in Eff is declared using the \verb|effect| keyword together with a type signature which forms the effect signature. One would define a \verb|Print| effect for printing strings as follows:



This tells us two things. To invoke the \verb|Print| effect we must provide a string as an argument and what we get back is something of type \verb|unit| after the effect is performed. We get to know no extra information about \verb|Print|.

Now that we have a \verb|Print| effect we can write down the mandatory Hello World example:
\begin{verbatim}
perform (Print "Hello World!")
\end{verbatim}

The \verb|perform| keyword is used to perform already declared effects. However, if we think about it, this piece of code still does
not make any sense as we don't know how to interpret \verb|Print "Hello World!"|.

\subsubsection{Handlers}

Handlers are used to assign meaning to effects. A handler is just a list of rules. A rule can be of three different types: a value rule, an effect rule or a finally rule.

\textbf{Value and effect rules}

The following handler provides an interpretation for the \verb|Print| effect. This handler converts the result of a computation to a 2-tuple where the first element is the original result and the second
element is a concatenation of the strings from all the \verb|Print| effects from the computation.

We use the value rule \verb|v -> (v, "")| to say what happens ``by default''. If the computation didn't print anything then we simply say that it printed the empty string.

\begin{verbatim}
let collect = handler
| v -> (v, "")
| effect (Print str) k ->
    (* Find out what the rest of the computation would print out *)
    let (result, s) = continue k () in
    (* Prefix the string printed by the rest of the computation 
       with `str` from this effect *)
    (result, str ^ s)
\end{verbatim}

The effect rule is a bit more involved. Effect rules have the form \verb|effect E(args) k| where \verb|k| is always bound to the ``rest of the computation'' which we call a continuation (and by convention
usually use the letter $k$ or $\kappa$ to denote it). The arguments of the effect \verb|E| are bound to \verb|args| via pattern matching. In the example above we first resume the computation by resuming the
computation via \verb|continue|. After we find out what the rest of the computation would print out, we prepend our current string to it.

Note that when we handle an effect of type $\alpha \to \beta$ we get a continuation of type $\beta \to \gamma$ where $\gamma$ is the type of the computation being enclosed in a \verb|with ... handle| block.

Value rules can be omitted in handler definitions, in which case they are assumed to be identities.

\textbf{Finally rules}

Finally rules are just syntactic sugar for \verb|let| wrappers around \verb|with ... handle| statements that act on the result of a handled computation, i.e.,
\begin{verbatim}
let h = handler
    | finally x -> x + x
in 
    with h handle c
\end{verbatim}
would be syntactic sugar for
\begin{verbatim}
let x = with h handle c in x + x.
\end{verbatim}

I would like to point out that this syntax is added only for convenience as it happens many times that the same transformations must be performed after a \verb|with ... handle| block \cite{bauer2015programming}. Hence \verb|finally| here does not provide the Java-like semantics where the computation in a finally block is ``guaranteed'' to be executed.

Finally rules can be omitted in handler definitions, in which case they are assumed to be identities.

\textbf{Handler types}

Handlers handling a computation of type $\alpha$ and giving a result of type $\beta$ are given the type $\alpha \Rightarrow \beta$.
\end{comment}

\begin{comment}
\subsection{TODOs}
\begin{itemize}
\item I haven't said anything about shallow and deep handlers
\item Comparing to haskell monads/monad transformers is risky -- need to explain these
\end{itemize}

\section{Papers}
\begin{itemize}
\item Delimited control \cite{kiselyov2010delimited}
\item Eff Directly in OCaml \cite{Kiselyov_2018}
\item Very good article about CPS compilation \cite{flanagan1993essence}
\item Bauer tutorial \cite{bauer2018algebraic}
\end{itemize}
\end{comment}

\ifstandalone
\bibliography{../bibliography}{}
\bibliographystyle{plain}
\fi

\end{document}